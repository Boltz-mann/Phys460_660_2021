{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time domain correlations\n",
    "\n",
    "Consider a pair of time dependent signals or stochastic (random) processes like the two shown in the figure below. They could be fluctuations in the current in a circuit, or the velocity or position a small (Brownian) particle subject to thermal fluctuations, or the deflection of your ear drum as you listen to this lecture, or... As you can see, this is a problem that arises in every corner of science and engineering.  \n",
    "\n",
    "![stochastic signals](./two_signals.png)\n",
    "\n",
    "In fact, these two traces are the angular deflection of a *tiny* mirror suspended from a very thin wire, measured by the reflection of a laser off of the mirror. The difference is that the upper trace was obtained at atmospheric pressure pressure, while the lower trace was obtained at about $10^{-7}$ atm. \n",
    "___\n",
    "**Question:** If you wanted to develop a physical model of this fluctuating mirror, what elements would you put in?\n",
    "___\n",
    "\n",
    "Are these signals random? Partially random and partially predictable? How do we know? What sort of mathematical analysis can answer this question?\n",
    "\n",
    "*Time series analysis* is the branch of statistical analysis that provides the tools to answer such questions. In the figure above, we might wonder whether there is any regularity in the position of the mirror --- is there some period of oscillation hidden in the signal? Does the timeseries have some *memory,* such that the position is correlated with earlier positions? How long is the memory?\n",
    "\n",
    "We can search for an answer in the *correlation function* of the signal. Let the signal of interest be denoted $x$. The time-correlation (or \"auto-correlation\") is defined as:\n",
    "\n",
    "$$C_x(t,t^\\prime) = \\langle x(t)x(t^\\prime)\\rangle \\tag{Eq. 1}$$\n",
    "\n",
    "The angle brackets mean \"average,\" which is required when we are dealing with observables that have both randomness and some inherent structure. (**Why?**) The average could be a\n",
    "* Ensemble average: Repeat the measurement (or simulation) many times, and then average over multiple realizations\n",
    "* Time average: Collect data for a long time, and treat different $(t,t^{\\prime})$ pairs as independent. \n",
    "\n",
    "**Time translation invariance** Here, we will consider (usually?) processes which are *ergodic.* **Question:** What does ergodic mean? \n",
    "\n",
    "* When a signal is invariant under translation in time, $C_x(t,t^\\prime)$ can only depend on the difference $(t-t^\\prime)$, which is often referred to as the \"lag time\". We can also pick $t^\\prime = 0$, and write $C_x(t) = \\langle x(t)x(0)\\rangle$.\n",
    "* It is common to subtract off the average value of $x$ if it is not zero, and compute instead the auto-correlation of the fluctuations about the average value:\n",
    "\n",
    "$$\\begin{align}\n",
    "C_x(t) & = \\langle \\left( x(t) - \\langle x\\rangle\\right) \\left( x(0) - \\langle x\\rangle\\right)\\rangle\\\\\n",
    " & = \\langle x(t)x(0) \\rangle - \\langle x \\rangle^2 \\tag{Eq. 2}\n",
    "\\end{align}$$\n",
    "\n",
    "To build some intuition into what this expression means, let's consider the case when $t=0$. We call this the \"equal time\" correlation function, or \"static\" correlation function, because it does not depend on time. Note that if there are $N$ observations in your time series, there are $N$ points at which you must compute the product inside the $\\langle...\\rangle$ --- $t$ is the *lag time* between two points:\n",
    "\n",
    "$$C_x(0) = \\langle x^2 \\rangle - \\langle x \\rangle^2 \\tag{Eq. 3}$$\n",
    "\n",
    "This is the *variance* of the data series. It's square root is the standard deviation. It is also common to normalize $C_x(t)$ by the variance, putting the $C_x(0)$ in the denominator like this:\n",
    "\n",
    "$$C_x(t) = \\frac{\\langle x(t)x(0) \\rangle - \\langle x \\rangle^2}{\\langle x^2 \\rangle - \\langle x \\rangle^2} \\tag{Eq. 4}$$\n",
    "\n",
    "If you use a built in routine to compute an autocorrelation, it might be computing Eq. 1, or Eq. 2, or Eq. 4...so you had better know the difference!\n",
    "\n",
    "At equal times, the signal is perfectly correlated with itself. As the time between observations goes to infinity ($t\\rightarrow\\infty$) the timeseries will (in most cases) forget its earlier values, and $C_x(t) \\rightarrow 0$. What happens in between contains an enormous amount of information about the signal. Often, given a signal of interest (or some data from a simulation), we might try and compute $C(t)$ directly in the time domain, and then see if it fits some simple model, like an exponential decay. In that case, the time constant of the exponential decay tells you how long (*on average*) it takes for the signal to \"forget\" where it was at an earlier time.\n",
    "\n",
    "*Aside:* You can think of the correlation function (when we are in discretized time, as we always are in the computer) as a *covariance matrix*, constrained by symmetry across the diagonal, positivity of the eigenvalues, and time-translation invariance. \n",
    "\n",
    "Let's use a completely random signal to explore some of these ideas and analysis. To build our timeseries I will draw a series of random numbers, uniformly distributed between $0$ and $1$. There are two loops. In the first loop, I generate the random timeseries and compute its average on the fly. Then in the second (double) loop I compute the correlation function. I'll compute all three versions: Eq. 1, 2, and 4.\n",
    "\n",
    "**Question:** How does Eq. 2 simplify for a case like this? (Random entries, no correlation between them.) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "## I'm going to write this code in a silly way, in order to make the math more obvious\n",
    "## There are *MUCH* smarter/faster ways to compute correlation functions...those will come later\n",
    "\n",
    "## This cell generates a simple sine wave, and superimposes some random noise on top, \n",
    "## and computes the autocorrelation of that signal\n",
    "\n",
    "\n",
    "N = 100\n",
    "amp = 1.0 ## amplitude of the signal. vary to see how the various \n",
    "          ## statistics change\n",
    "rand_signal = []\n",
    "time = []\n",
    "total = 0 ## this will store the sum of the signal as it is generated \n",
    "          ## and at the end of the first loop I'll comnpute the average\n",
    "\n",
    "for i in range(0,N):\n",
    "    \n",
    "    ##NOTE: here is our first use of a pseudo random number, which calls the \"random\" \n",
    "    ## function from the python lib \"random.\" This routine is based on the Mersenne twister\n",
    "    tmp1 =  amp*rand.random()\n",
    "    rand_signal.append(tmp1) \n",
    "    time.append(i)\n",
    "    total = total + tmp1\n",
    "\n",
    "sig_avg = total/N\n",
    "\n",
    "## this list will hold the autocorrelation functions\n",
    "C_rand = []  #Eq. 1\n",
    "C_rand_dev = [] #Eq. 2\n",
    "C_normed = [] #Eq. 4\n",
    "\n",
    "# delta is the \"lag time\"\n",
    "for delta in range (0,N-10):\n",
    "    ## total keeps the sum for each time lag, \n",
    "    ## incr counts how many terms contribute to the sum\n",
    "    total = 0 # This will store the sums as we compute Eq. 1\n",
    "    total2 = 0 # This will store the sums as we compute Eq. 2\n",
    "    incr = 0\n",
    "    for i in range (1,N-delta):\n",
    "        incr = incr + 1\n",
    "        # the product at each pair of data lagged by delta t\n",
    "        total = total + rand_signal[i]*rand_signal[i+delta]\n",
    "        total2 = total2 + (rand_signal[i] - sig_avg)*(rand_signal[i+delta] - sig_avg)\n",
    "    \n",
    "    ## here we compute the average (this is dumb when the sum has lots of terms)\n",
    "    avg = total/incr\n",
    "    avg2 = total2/incr\n",
    "    C_rand.append(avg)\n",
    "    C_rand_dev.append(avg2)\n",
    "    \n",
    "# cant figure out how to scale the axes of the C(t) plot\n",
    "M_C = len(C_rand)\n",
    "x_axis = np.linspace(0,M_C,M_C)\n",
    "\n",
    "var = C_rand[0]  #the variance: <x**2>\n",
    "denom = var - sig_avg*sig_avg # compute <x**2> - <x>**2\n",
    "\n",
    "#now fill the elements of C_normed with Eq. 4\n",
    "for i in range (0,M_C):\n",
    "    tmp = C_rand_dev[i]/denom\n",
    "    C_normed.append(tmp)\n",
    "\n",
    "print(\"<x>: %8.4f\" % sig_avg)\n",
    "print(\"<x**2>: %8.4f\" % var)\n",
    "print (\"<x**2>-<x>**2: %8.4f\" % denom)\n",
    "print(\"C(t=0) Eq. 1: %8.4f\" % C_rand[0])\n",
    "print(\"C(t=0) Eq. 2: %8.4f\" % C_rand_dev[0])\n",
    "print(\"C(t=0) Eq. 4: %8.4f\" % C_normed[0])\n",
    "\n",
    "plt.figure(1, figsize=(10, 3))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax = plt.subplot(221)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('signal')\n",
    "plt.plot(time,rand_signal)\n",
    "#plt.plot(time,noisy_periodic)\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "ax2.set_xlabel('lag time')\n",
    "ax2.set_ylabel('C(t) (Eq. 1)')\n",
    "plt.plot(x_axis,C_rand)\n",
    "\n",
    "ax3 = plt.subplot(223)\n",
    "ax3.set_xlabel('time')\n",
    "ax3.set_ylabel('C(t) (Eq. 2)')\n",
    "plt.plot(x_axis,C_rand_dev)\n",
    "#plt.plot(x_axis,C_normed)\n",
    "\n",
    "ax4 = plt.subplot(224)\n",
    "ax4.set_xlabel('time')\n",
    "ax4.set_ylabel('C(t) (Eq. 4)')\n",
    "plt.plot(x_axis,C_normed)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the loop that computes the correlation function. The data series is just a numbered sequence of values:\n",
    "![time series](./timeseries.jpg)\n",
    "\n",
    "To compute the $t=0$ entry in Eq. 1, we form the product of every value with itself, sum them, and then average:\n",
    "$$C(t=0) = \\frac{\\left(x_0x_0+x_1x_1+x_2x_2+...+x_{N-1}x_{N-1} \\right)}{N}$$\n",
    "\n",
    "Then we compute the $t=1$ lag time:\n",
    "$$C(t=1) = \\frac{\\left(x_0x_1+x_1x_2+x_2x_3+...+x_{N-2}x_{N-1} \\right)}{N-1}$$\n",
    "\n",
    "Then $t=2$:\n",
    "$$C(t=2) = \\frac{\\left(x_0x_2+x_1x_3+x_2x_4+...+x_{N-3}x_{N-1} \\right)}{N-2}$$\n",
    "\n",
    "As we go to longer and longer lag times, there are fewer and fewer pairs of values to average together, because the time series is finite in length. This problem gets extreme at the longest lag times:\n",
    "\n",
    "Lag time $t=N-2$:\n",
    "$$C(t=N-2) = \\frac{\\left(x_0x_{N-2}+x_1x_N-1\\right)}{2}$$\n",
    "\n",
    "In the code we therefore implement a double loop --- one loop for the lag time, and the second for the starting position:\n",
    "\n",
    "    for lag=0, lag < N-1\n",
    "        for j = 0, j < N:\n",
    "            compute each product\n",
    "            add to total\n",
    "            ## exit j loop ##\n",
    "        compute avg\n",
    "\n",
    "**Discussion (important!):** Notice the number of operations required to compute $C_x(t)$, and think about what happens if you have a very long timeseries, and just naively compute the whole damn thing by brute force!\n",
    "\n",
    "### A signal with some memory\n",
    "Now let's take a look at a more random signal, but one which still has structure. First we will generate a sequence of *independent* Gaussian distributed values, and compute the autocorrelation of this \"signal.\" Since they are independent, there should be no correlation, just like before. The only difference between this random sequence and the one above is that the random numbers in the last code cell were *uniformly* distributed, and these are *Gaussian* distributed. \n",
    "\n",
    "Then we will generate a sequence of Gaussian distributed numbers, but we will make them correlated, by making each new member of the sequence dependent on the previous one. This algorithm (which I shamelessly stole from Markus Deserno's (CMU Physics) website) looks like\n",
    "\n",
    "$$x_{N+1} = fx_N + \\sqrt{(1-f^2)}g_{N+1}$$\n",
    "\n",
    "where $g_N$ is a random number drawn from a Gaussian distribution of zero mean an unit variance, and $f$ introduce the memory function via a timescale $\\tau$: $f=e^{-1/\\tau}$. So if we simulate such a sequence and compute its correlation function, we expect to get an exponential decay on a timescale of $\\tau$.\n",
    "\n",
    "*Aside:* This type of noise arises often in the context of the motion of a Brownian particle, and is called \"Ornstein-Uhlenbeck\" noise.\n",
    "\n",
    "*Aside2:* Even though most commonly used numerical libraries have built in (pseudo) random number generators, generating sequences of random numbers numerically is not trivial. In fact, most such generators are not really random at all, but deterministic (same seed, same sequence) and repeating (but good generators have very long periods). The random numbers produced by Python's \"random\" library are based on the Mersenne Twister algorithm (Matsumoto and Nishimura, 1998) which has a period of $2^{19937} - 1$. Python's rand documentation contains a warning about using these sequences for cryptographic applications, for obvious reasons. \n",
    "\n",
    "*Aside3:* The Mersenne Twister and its cousins all generate *uniformly* distributed real numbers in the interval \\[0,1). If you want Gaussian distributed (or Poisson, or Weibull, or...) then you have to apply a transformation to the uniform i.i.d. (\"independent and identically distributed\") numbers. These are already implemented in Python as well. But when I was your age, we coded our own transformations while walking uphill through the snow. On punchcards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Here I use an algorithm that Markus Deserno wrote up in 2002. \n",
    "## It can be found at the CMU biophysics website\n",
    "\n",
    "tau = 2.0 # vary the timescale to see that the correlation time changes \n",
    "        # as expected\n",
    "f =  np.exp(-1.0/tau)\n",
    "g_scale = np.sqrt(1 - f**2)\n",
    "#print(f, g_scale)\n",
    "\n",
    "# for debugging, use the same sequence of rands\n",
    "#rand.seed(2)\n",
    "\n",
    "uni_sig = []\n",
    "corr_sig = []\n",
    "t = []\n",
    "s0 = rand.gauss(0.0, 1.0)\n",
    "uni_sig.append(s0)\n",
    "corr_sig.append(s0)\n",
    "t.append(0)\n",
    "\n",
    "# the loop generates the sequence (\"corr_sig\")of correlated random numbers according to Deserno's algorithm\n",
    "# we will also generate a sequence of Gaussian distributed numbers with no \n",
    "# correlation for comparison (\"uni_sig\")\n",
    "for i in range(0,10000):\n",
    "    tmp2 = f*corr_sig[i-1] + g_scale*rand.gauss(0.0, 1.0)\n",
    "    tmp = rand.gauss(0.0, 1.0)\n",
    "    uni_sig.append(tmp)\n",
    "    corr_sig.append(tmp2)\n",
    "    t.append(i)\n",
    "\n",
    "# calculate the autocorrelation function. \n",
    "# for some reason, every other value of the sequence is anti-correlated. \n",
    "# this must be \"real,\" (ie, not a bug), but it disctracts from the main point \n",
    "# which is to show that the auto correlation captures the exponential decay in the memory\n",
    "\n",
    "uni_Ct = []\n",
    "corr_Ct= []\n",
    "M = len(corr_sig)\n",
    "for delta in range (0,50,2):\n",
    "    total = 0\n",
    "    total2 = 0\n",
    "    incr = 0\n",
    "    for i in range (1,M-delta):\n",
    "        incr = incr + 1\n",
    "        total = total + uni_sig[i]*uni_sig[i+delta]\n",
    "        total2 = total2 + corr_sig[i]*corr_sig[i+delta]\n",
    "        \n",
    "    avg = total/incr\n",
    "    avg2 = total2/incr\n",
    "    uni_Ct.append(avg)\n",
    "    corr_Ct.append(avg2)\n",
    "\n",
    "# normalize the auto correlations by the variance so they start from 1\n",
    "# ie, we plot Eq. 4\n",
    "M_C = len(uni_Ct)\n",
    "for i in range(0,M_C):\n",
    "    uni_Ct[i] = uni_Ct[i]/uni_Ct[0]\n",
    "    corr_Ct[i] = corr_Ct[i]/corr_Ct[0]\n",
    "\n",
    "x2_axis = np.linspace(0,M_C,M_C)\n",
    "calc_C = np.exp(-1.0*x2_axis/tau)\n",
    "\n",
    "## start with just the uncorrelated signal and it's autocorr\n",
    "## then add the correlated signal\n",
    "## series2 is the correlated signal\n",
    "plt.figure(1, figsize=(8.5, 4))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax = plt.subplot(121)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('signal')\n",
    "ax.set_xlim(right=100)\n",
    "plt.plot(t,uni_sig)\n",
    "plt.plot(t,corr_sig)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.set_xlabel('time')\n",
    "ax2.set_ylabel('C(t)')\n",
    "plt.plot(x2_axis,uni_Ct)\n",
    "plt.plot(x2_axis,corr_Ct)\n",
    "plt.plot(x2_axis,calc_C)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
